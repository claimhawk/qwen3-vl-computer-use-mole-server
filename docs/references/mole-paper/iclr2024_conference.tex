
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{marvosym}
\usepackage{url}
\usepackage{inconsolata}
\usepackage{url}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{arydshln} % For dashed lines
\usepackage{amssymb}
\usepackage{subfig}
\usepackage{diagbox}
\usepackage{epsfig}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{extpfeil}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfig}
\usepackage{diagbox}
\usepackage{epsfig}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{extpfeil}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{color,xcolor,colortbl}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{rotating}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}            % simple URL typesetting
\usepackage{fmtcount}
% \usepackage{lmodern}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{epsfig}
\usepackage[normalem]{ulem}  
\usepackage{booktabs}
\usepackage{tcolorbox}
\usepackage[colorlinks=true,citecolor=.]{hyperref}
% \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
% \newcommand{\tx}[1]{``\textit{#1}''}
% \newcommand{\sptk}[1]{\texttt{[#1]}}
% \newcommand{\eqform}[1]{Equation~(\ref{#1})}
\newcommand\hc{\rowcolor{teal!20}}
% \usepackage{epsfig}
% \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
% \newcommand{\tx}[1]{``\textit{#1}''}
% \newcommand{\sptk}[1]{\texttt{[#1]}}
% \newcommand{\eqform}[1]{Equation~(\ref{#1})}
% \input{math_commands.tex}
% \input{settings.tex}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{cleveref}
\newcommand\ha{ \rowcolor{blue!0}}
\newcommand\hb{ \rowcolor{teal!7}}
\newcommand\hp{ \rowcolor{purple!10}}
% \newcommand{\crefrangeconjunction}{--}
% \newcommand{\chib}{{\boldsymbol {\chi}}}
% \newcommand{\xib}{{\boldsymbol {\xi}}}
% \newcommand{\Hbc}{{\boldsymbol{\mathcal H}}}
% \newcommand{\Xbc}{{\boldsymbol{\mathcal X}}}
% \newcommand{\Kbc}{{\boldsymbol{\mathcal K}}}
% \newcommand{\Mbc}{{\boldsymbol{\mathcal M}}}
% \newcommand{\Kc}{{{\mathcal K}}}
% \newcommand{\Pc}{{{\mathcal P}}}
% \newcommand{\Ec}{{{\mathcal E}}}
% \newcommand{\Dc}{{{\mathcal D}}}
% \newcommand{\xb}{{\boldsymbol x}}
% \newcommand{\yb}{{\boldsymbol y}}
% \newcommand{\zb}{{\boldsymbol z}}
\usepackage{caption}
\usepackage{subcaption}
\newcommand\our{\textsc{MoLE}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
% \newcommand{\RomanNumeralCaps}[1]
%     {\MakeUppercase{\romannumeral #1}}

\title{Mixture of LoRA Experts}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }
\author{Xun Wu$^{1,2}$\thanks{Contribution during internship at Microsoft. \textsuperscript{\Letter} Corresponding Author.},\quad Shaohan Huang$^{1,}$\textsuperscript{\Letter},\quad Furu Wei$^1$\\
$^1$Microsoft Research Asia \quad $^2$Tsinghua Univeristy\\
\small{\texttt{wuxun21@mails.tsinghua.edu.cn;} ~\texttt{\{shaohanh,~fuwei\}@microsoft.com}}\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

% \newcommand{\fix}{\marginpar{FIX}}
% \newcommand{\new}{\marginpar{NEW}}
% \setsansfont{Verdana}
\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Low-Rank Adaptation (LoRA)~\citep{lora} has emerged as a pivotal technique for fine-tuning large pre-trained models, renowned for its efficacy across a wide array of tasks. The modular architecture of LoRA has catalyzed further research into the synergistic composition of multiple trained LoRAs, aiming to amplify performance across various tasks. However, the effective composition of these trained LoRAs presents a formidable challenge:
%
% (1) Linear arithmetic composition method may lead to the loss of the generative capabilities inherent in the original pre-trained model or the distinctive attributes of the trained LoRAs, resulting in suboptimal outcomes.
% %
% (2) Reference tuning-based composition method exhibits limitations in terms of the necessary adaptability for effectively composing multiple LoRAs and incurs significant costs due to retrain a sizable model.
(1) Linear arithmetic composition can lead to the diminution of the generative capabilities inherent in the original pre-trained models or the distinctive attributes of the individually trained LoRAs, potentially resulting in suboptimal outcomes.
%
(2) Reference tuning-based composition exhibits limitations in adaptability and incurs significant computational costs due to the requirements to retrain a large model.
%
In response to these challenges, we propose \textbf{M}ixture \textbf{o}f \textbf{L}oRA \textbf{E}xperts (\textbf{\our{}}). 
%
\our{} treats each layer of trained LoRAs as a distinct expert and implements hierarchical weight control by integrating a learnable gating function within each layer to learn optimal composition weights tailored specifically to the objectives of a given domain. 
%
\our{} not only demonstrates enhanced performance in LoRA composition but also preserves the essential flexibility necessary for effective composition of trained LoRAs with minimal computational overhead. Extensive experiments conducted in both Natural Language Processing (NLP) and Vision \& Language (V\&L) domains validate the effects of \our{}. Our code are available at~\href{https://github.com/yushuiwx/MoLE.git}{\texttt{https://github.com/yushuiwx/MoLE.git}}.
\end{abstract}

\section{Introduction}
%
\begin{wrapfigure}{R}{0.45\textwidth}
\vspace{-8mm}
\centering
\includegraphics[width=\linewidth]{worflow.pdf}
\vspace{-6mm}
\caption{\textbf{Workflow of \our{}}. In the training phase, \our{} predicts weights for multiple LoRAs. In the inference phase, \our{} can allocate weights to multiple LoRAs, or, without altering the gating weights, achieve a more flexible LoRA composition by masking out undesired LoRAs and recalculating and distributing weights proportionally.}
\label{fig:workflow}
\vspace{-5mm}
\end{wrapfigure}
%
Recent advances in deep learning have been driven by large-scale pre-trained models such as OPT~\citep{opt}, LLaMA~\citep{llama} in the Natural Language Processing~(NLP) domain and CLIP~\citep{clip}, DALLÂ·E 2~\citep{dell2} in the Vision \& Language~(V\&L) domain.
%
These models show outstanding performance across various tasks when fine-tuned on down-stream datasets, but their increasing size entails significant computational costs for full fine-tuning.
%
\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{LoRA_fusion.pdf}
\\
\makebox[0.32\linewidth]{(a)}
\makebox[0.32\linewidth]{(b)}
\makebox[0.32\linewidth]{(c)}
\vspace{-2mm}
\caption{Overview of LoRA composition methods:~(a) Linear arithmetic composition (Eq.\ref{Eq.Normalize-Combination}), which commonly applies the same composition weight $\boldsymbol{W}_i$ to all layers of the $i^{th}$ LoRA. (b) Reference tuning-based composition involves retraining a large model by integrating outputs from multiple LoRAs using manually-crafted mask information. (c) Our \our, which learns a distribution $\Upsilon^{j}$ for the $j^{th}$ layer of LoRAs to determine the composition weight $\boldsymbol{W}^{j}_i$.}
\label{fig:main_motivation}
\vspace{-3mm}
\end{figure*}
%
To mitigate this, LoRA~\citep{lora} is introduced.
%
By freezing the pretrained model weights and injecting trainable rank decomposition matrices, LoRA is proven to be an effective fine-tuning methodology in scenarios with constrained computational resources~\citep{lester2021power,an2022input}.

While LoRA serves as plug-and-play plugins for pre-trained models, recent initiatives explores the composition of separate trained LoRAs to achieve joint generation of learned characteristics~\citep{huang2023lorahub,zhang2023composing,ruiz2023dreambooth}. However, these efforts may encounter several challenges.
%
As shown in Figure~\ref{fig:main_motivation} (a), linear arithmetic composition~\citep{zhang2023composing,huang2023lorahub,han2023svdiff} composes trained LoRAs directly. However, composing multiple LoRAs (typically $\geq$ 3) can impair the generative performance of pre-trained models. To mitigate this, weight normalization is applied prior to the composition, but may erase the unique characteristics of individual trained LoRAs as the composing weight of each LoRA is reduced (refer to Observation 1 in \S~\ref{Sec:Observation}). 
%
Another approach, as depicted in Figure~\ref{fig:main_motivation} (b), known as reference tuning-based composition~\citep{gu2023mix}, is tailored for the V\&L domain and achieves superior performance. However, it is limited in terms of LoRA flexibility due to the utilization of manually-designed masks and involves substantial training costs, necessitating a full model retraining.
%
In light of the above situation, an important question arises:

\begin{tcolorbox}[colback=gray!20, colframe=gray!50, sharp corners, center title]
\centering
\textit{\small How can multiple trained LoRAs be composed dynamically and efficiently, while preserving \\ all their individual characteristics?}
\end{tcolorbox}

% \begin{center}
% \vspace{-0.5mm}
% \emph{How can multiple trained LoRAs be composed dynamically and efficiently, while preserving all their individual characteristics?}
% \vspace{-0.5mm}
% \end{center}

To address that issues, we introduce \textbf{M}ixture \textbf{o}f \textbf{L}oRA \textbf{E}xperts~(\textbf{\our{}}). Recognizing that individual layers of a trained LoRA exhibit distinct characteristics, which collectively define the overall characteristic of the trained LoRA (refer to Observation 2 in \S~\ref{Sec:Observation}), \our{} involves modulating the weights of different trained LoRAs within each layer, which we refer to as ``hierarchical weight contro''. As shown in Figure~\ref{fig:main_motivation}~(c), \our{} views each layer of trained LoRAs as a individual expert and incorporates a gating function within each layer to learn the optimal composition weights based on a specified domain objective. This dynamically enhances desirable characteristics while mitigating less favorable ones, ultimately achieving a more effective composition of LoRAs and prevents the loss of desirable LoRA characteristics that may occur in linear arithmetic composition.
%

Additionally, unlike reference tuning-based composition~\citep{gu2023mix}, our \our{} maintains flexibility in composing multiple trained LoRAs with reduced computational costs.
%
As the workflow of \our{} shown in Figure~\ref{fig:workflow}, during training, \our{} learns the gating function for multiple trained LoRAs and keep all other parameters frozen, resulting in minimal computational costs. During inference, \our{} has two inference modes:
In the first mode, \our{} utilizes all trained LoRAs with the learned gating function, preserving their individual characteristics with allocated weights.
%
During the second mode, \our{} allows manual masking of unwanted LoRAs and recalculates and distributes weights proportionally without the need for retraining.
%
These two modes enable \our{} to adapt to different scenarios, providing a versatile and flexible approach for effective LoRA composition.

We validate the effects of \our{} in both NLP and V\&L domains. Our findings, encompassing both qualitative and quantitative results, demonstrate that \our{} outperforms existing LoRA composition approaches.
%
The contributions of our paper are the following:
% \vspace{-1mm}
\begin{itemize}
\vspace{-1mm}
    \item We introduce a significant and intricate problem: how to dynamically and efficiently compose multiple trained LoRAs while preserving all their individual characteristics, to further investigate the applicability of LoRA in real-world scenarios.
    \item We introduce Mixture of LoRA Experts (\our{}), a method that achieves a more efficient and flexible composition of multiple trained LoRAs by employing hierarchical weight control through learnable gating functions within each layer of trained LoRAs.
    \item Extensive experiments on both V\&L and NLP domain demonstrate that \our{} can enhance LoRA composition performance and mitigates issues associated with existing composition methods.
\end{itemize}

\section{Background}

\subsection{LoRAs Composition}
\label{Sec:LoRA Merging}
LoRA~\citep{lora} is a parameter-efficient fine-tuning method to adapt large models to novel tasks and shows superior performance~\citep{lora,huang2023lorahub,zhang2023composing,sung2022vl}.
%
In practical applications, a individual LoRA often fall short of meeting user expectations. A common solution is to compose multiple trained LoRAs, each specialized in specific aspects (e.g., clothing or facial features), with the aim of creating a comprehensive character representation.
%
Research on LoRA composition is limited and primarily concentrates on two distinct methodologies as follows:

\noindent\textbf{Linear arithmetic composition}. 
%
As shown in Figure~\ref{fig:main_motivation}~(a), the most commonly employed composition method is directly composing multiple LoRAs,~i.e., 
%
\begin{equation}
\vspace{-1mm}
\hat{\boldsymbol{W}} = \boldsymbol{W} + \sum_{i=1}^N \Delta \boldsymbol{W}_i, 
\vspace{-1mm}
\label{Eq.Direcly-Combination}
\end{equation}
%
where $\boldsymbol{W}$ indicates the original parameter of pre-trained model and $\Delta\boldsymbol{W}_i$ denotes the $i^{th}$ trained LoRA.
%
However, this manner may affect the original weight $\boldsymbol{W}$ when $N$ increasing, thereby diminishing the model's generative capabilities.
%
So, it is common practice to normalize the composition weights, termed as normalized linear arithmetic composition, ~i.e., 
\begin{equation}
\vspace{-1mm}
\hat{\boldsymbol{W}} = \boldsymbol{W} + \sum_{i=1}^N w_i \cdot \Delta \boldsymbol{W}_i,
\vspace{-1mm}
\label{Eq.Normalize-Combination}
\end{equation}
where $\sum_{i=1}^N w_i = 1$. This manner prevents any adverse impact on the embedding of the original model, but leading to the loss of individual LoRA characteristics, as the composing weight $w_i$ for each trained LoRA is reduced~\citep{gu2023mix}. 

In NLP domain, PEMs~\citep{zhang2023composing} first  define arithmetic operators for LoRA, and explore the effectiveness of composing multiple LoRAs in several scenarios. LoRAhub~\citep{huang2023lorahub} utilizes a gradient-free manner to estimate the composition weights of trained LoRAs and achieves adaptable performance on unseen tasks. In V\&L domain, SVDiff~\citep{han2023svdiff} introduces a arithmetic-based manner to compose multiple visual concepts into a single image.
%
%

\noindent\textbf{Reference tuning-based composition}. As shown in Figure~\ref{fig:main_motivation}~(b), reference tuning-based composition~\citep{gu2023mix} tackles the limitations of linear arithmetic composition by introducing gradient fusion and controllable sampling. However, it suffers from compositional inflexibility due to manually designed masks, which necessitates retraining when incorporating different LoRAs or creating new masks. Moreover, this approach entails retraining large models, resulting in substantial computational costs.
%

It is important to note that reference tuning-based composition relies on position masks, which distinguishes it from our model. Consequently, direct comparisons may not be appropriate due to the fundamentally different underlying principles. Therefore, our primary focus in this paper is to compare \our{} with linear arithmetic composition.

\subsection{Mixture-of-Experts}
Mixture-of-Experts~(MoE)~\citep{moec} is a promising approach to scale up the number of parameters within the same computational bounds. 
%
Different from standard transformer models, each MoE layer consists of $N$ independent feed-forward networks $\{\boldsymbol{E}_i\}^{N}_{i=0}$ as the experts, along with a gating function $\alpha \left(\cdot\right)$ to model a probability distribution indicating the weights over these experts' outputs.
%
For the hidden representation $\boldsymbol{h} \in \mathbb{R}^{d}$ of input token, the gate value of routing $\boldsymbol{h}$ to expert $\boldsymbol{E}_i$ is denoted as:
%
\begin{equation}
\vspace{-2mm}
\alpha \left(\boldsymbol{E}_i\right) = \exp\left(\boldsymbol{h}\cdot\boldsymbol{e}_i\right) / \sum_{j=0}^{N}\exp\left(\boldsymbol{h}\cdot\boldsymbol{e}_j\right),
% \vspace{0.5mm}
\end{equation}
%
%
where $\boldsymbol{e}_i$ denotes the trainable embedding of $\boldsymbol{E}_i$. Then, the corresponding $k$ experts, according to the top-$k$ gated values, are activated and the output $\boldsymbol{O}$ of the MoE layer is
\begin{equation}
\vspace{-2mm}
\boldsymbol{O} = \boldsymbol{h} + \sum_{i=0}^{N}\alpha \left(\boldsymbol{E}_i\right) \cdot \boldsymbol{E}_i\left(\boldsymbol{h}\right).
\vspace{-2mm}
\end{equation}
%

\begin{figure*}[!tb]
\centering
\includegraphics[width=\linewidth]{main_motivations.pdf}
% \\
% \makebox[0.49\textwidth]{\footnotesize \RomanNumeralCaps{1}}
% \makebox[0.49\textwidth]{\footnotesize \RomanNumeralCaps{2}}
\vspace{-2mm}
\caption{\textbf{Left:} Results of (a) linear arithmetic composition~(Eq.~\ref{Eq.Direcly-Combination}) and~(b) normalized linear arithmetic composition~(Eq.~\ref{Eq.Normalize-Combination}) based on Dreambooth~\citep{ruiz2023dreambooth}. \textbf{Right:} Visualization of the effects for different layers in LoRA by selectively activating specific parameters from the network, moving from the beginning to the end. 
% \RomanNumeralCaps{3} - \RomanNumeralCaps{4}. Visualization of the motivation experiments in the NLP domain.
}
\label{fig:Observation}
\vspace{-4mm}
\end{figure*}

\section{Method}
% %
In this section, we first introduce some motivating observations in \S~\ref{Sec:Observation}. Then, we introduce the structure details and training objectives of \our{} in \S~\ref{Sec:MOLE} and \S~\ref{Sec:training}, respectively.
% %
\subsection{Motivating Observation}
\label{Sec:Observation}
%

\begin{tcolorbox}[colback=gray!20, colframe=gray!50, sharp corners, center title]
\textit{\small \textbf{Observation 1}: \textit{Directly composing multiple trained LoRAs~(Eq.~\ref{Eq.Direcly-Combination}) impacts the model's generative ability, whereas applying weight normalization~(Eq.~\ref{Eq.Normalize-Combination}) preserves this capacity but may sacrifice LoRA characteristics.}}
\end{tcolorbox}

% \textbf{Observation 1}: \textit{Directly composing multiple trained LoRAs~(Eq.~\ref{Eq.Direcly-Combination}) impacts the model's generative ability, whereas applying weight normalization~(Eq.~\ref{Eq.Normalize-Combination}) preserves this capacity but may sacrifice LoRA characteristics.}

Specifically, in V\&L domain, as depicted in left of Figure~\ref{fig:Observation}, we observe that directly composing multiple trained LoRAs into the original embedding led to significant parameter variations, resulting in meaningless output. Furthermore, when normalization was applied, some of the original characteristics of these trained LoRAs are indeed compromised. These observations align with those elaborated upon in~\citep{gu2023mix}.

In NLP domain, when composing four or more LoRAs within the FLAN-T5~\citep{flant5} model, we observed that the model's output became disordered.
%
Furthermore, implementing weight normalization for LoRAs trained across five datasets, as presented in Table~\ref{tb: NLP_motivation1}, led to a decreased performance of the composition model. This suggests that while weight normalization preserves generative capacity, it adversely affects the intrinsic qualities of these trained LoRAs.
%

\begin{tcolorbox}[colback=gray!20, colframe=gray!50, sharp corners, center title]
\textit{\small \textbf{Observation 2}: \textit{Individual layers of a trained LoRA exhibit unique traits, which cumulatively define the LoRA's overall attributes.}}
\end{tcolorbox}

% \textbf{Observation 2}: \textit{Individual layers of a trained LoRA exhibit unique traits, which cumulatively define the LoRA's overall attributes.}

Inspired by the findings of~\citep{voynov2023p+}, which revealed that different layers in text-to-image models govern various attributes, such as style and color, we investigate the features learned by different layers within LoRA.
%
In V\&L domain, as illustrated in the right of Figure~\ref{fig:Observation}, we observed that different layers of LoRA encode distinct features, such as dog coat color and facial features.
%
In NLP domain, we trained a single LoRA on a combined dataset comprising ANLI-R1~\citep{ANLI}, ANLI-R2~\citep{ANLI}, and QNLI~\citep{QNLI} datasets, as depicted in Table~\ref{tb: NLP_motivation2}. Notably, when evaluated on these sub-datasets, we observed significant variations in performance across different layers of this LoRA. Specifically, the layers ranging from 0\% to 20\% performed best on QNLI, the layers spanning from 40\% to 60\% excelled on ANLI-R2, and the layers covering 80\% to 100\% outperformed others on ANLI-R1.
%
\begin{wrapfigure}{R}{0.45\textwidth}
\vspace{-10mm}
\centering
\includegraphics[width=\linewidth]{main_MoLE.pdf}
\vspace{-6mm}
\caption{\textbf{Illustration of proposed \our{}}. \our{} employs a learnable gating function that utilizes the outputs of multiple LoRAs at each layer to determine composition weights.}
\label{fig:Mole_structure}
\vspace{-13mm}
\end{wrapfigure}
%
This observation inspires that we can dynamically optimizes the layer-specific weights according to a defined domain objective, enhancing desirable characteristics while suppressing less favorable ones, thereby achieving a more effective composition of trained LoRAs.

\subsection{Mixture of Lora Experts}
\label{Sec:MOLE}
%
Drawing inspiration from above observations, we introduce the Mixture of LoRA Experts.

Referring to Figure~\ref{fig:Mole_structure}, consider a transformer block within the pre-trained model, parameterized by $\theta$ (encompassing both the multi-head attention layer and the feed-forward neural network), and a set of corresponding trained LoRAs $\Omega = \{\Delta\theta_i\}^{N}_{i=0}$ where $N$ represents the number of trained LoRA candidates, when given a input $\boldsymbol{x} \in \mathbb{R}^{L\times d}$, the output of the pre-trained model block $\theta$ is presented as $\boldsymbol{F}_{\theta} \in \mathbb{R}^{L\times d}$:
%
\begin{align}
\boldsymbol{x}^{'}_{\theta} &= \boldsymbol{x} + f_{\text{Attn}}\Big(\text{LN}\big(\boldsymbol{x}\big)\big|\theta\Big), \\
\boldsymbol{F}_{\theta}\big(\boldsymbol{x}\big) &= \boldsymbol{x}^{'}_{\theta} + f_{\text{FFN}}\Big(\text{LN}\big(\boldsymbol{x}^{'}_{\theta}\big)\big|\theta\Big),
\end{align}
%
where $L$ and $d$ indicate the sequence length and the dimension of $\boldsymbol{x}$, respectively. $f_{\text{Attn}}\left(\cdot\right)$ and $f_{\text{FFN}}\left(\cdot\right)$ denotes the multi-head attention layer and feed-forward neural network, respectively. LN refers to layer normalization.
%
The output of each LoRA is presented as $\boldsymbol{E}_{\Delta\theta_i}\left(\boldsymbol{x}\right) \in \mathbb{R}^{L\times d}$,
%
\begin{align}
\boldsymbol{x}^{'}_{\Delta\theta_i} &= \boldsymbol{x} + f_{\text{Attn}}\Big(\text{LN}\big(\boldsymbol{x}\big)\big|\Delta\theta_i\Big), \\
\boldsymbol{E}_{\Delta\theta_i}\big(\boldsymbol{x}\big) &= \boldsymbol{x}^{'}_{\Delta\theta_i} + f_{\text{FFN}}\Big(\text{LN}\big(\boldsymbol{x}^{'}_{\Delta\theta_i}\big)\big|\Delta\theta_i\Big).
\end{align}
%
%

After that, \our{} applies a learnable gating function $\mathcal{G}\left(\cdot\right)$ to model the optimal distribution of composition weights for outputs of these trained LoRAs.
%
Specifically, by taking $\{\boldsymbol{E}_{\Delta\theta_i}\left(\boldsymbol{x}\right)\}_{i=0}^{N}$ as input, $\mathcal{G}\left(\cdot\right)$ first apply concatenation (denoted as $\oplus$) and normalization (for training stability),~i.e.
%
\begin{equation}
    \boldsymbol{E}_{\Omega}\left(\boldsymbol{x}\right) =  \text{Normalization}\Big(\boldsymbol{E}_{\Delta\theta_0}\left(\boldsymbol{x}\right)\, \oplus \,\ldots\, \oplus \,\boldsymbol{E}_{\Delta\theta_{N-1}}\left(\boldsymbol{x}\right)\Big),
\end{equation}
where $\boldsymbol{E}_{\Omega}\left(\boldsymbol{x}\right) \in \mathbb{R}^{\xi}$ and $\xi = N\times L\times d$. $\oplus$ indicates the concatenation operation.
%
Then we flatten and reduce the $\boldsymbol{E}_{\Omega}\left(\boldsymbol{x}\right)$ to $N$-dimensions by a dot-product operation with the learnable parameter $\boldsymbol{e} \in \mathbb{R}^{\xi \times N}$ in the gating function $\mathcal{G}\left(\cdot\right)$, 
\begin{equation}
\varepsilon = \text{Flatten}\Big(\boldsymbol{E}_{\Omega}\left(\boldsymbol{x}\right)\Big)^{\top} \cdot \boldsymbol{e}, \quad \varepsilon \in \mathbb{R}^{N},
\end{equation}
%
The gate value for each LoRA is computed as
%
\begin{equation}
\label{EQ. temp}
\mathcal{G}\big(\varepsilon_i\big) = \frac{\exp\big(\varepsilon_i / \tau\big)}{\sum_{j=1}^{N} \exp\big(\varepsilon_j / \tau\big)},
\end{equation}
%
the temperature scalar $\tau$ is learnable.
%
The final output $\Tilde{\boldsymbol{E}}_{\Omega}(\boldsymbol{x})$ of the gating function $\mathcal{G}\left(\cdot\right)$ is obtained by multiplying the output of each LoRA expert with the corresponding gating values, presented as
%
\begin{equation}
\Tilde{\boldsymbol{E}}_{\Omega}(\boldsymbol{x}) = \sum_{i=0}^{N}\mathcal{G}_i\left(\varepsilon_i\right)\cdot\boldsymbol{E}_{\Delta\theta_i}\left(\boldsymbol{x}\right),
\end{equation}
%
in which $\Tilde{\boldsymbol{E}}_{\Omega}(\boldsymbol{x}) \in \mathbb{R}^{L\times d}$ and $\mathcal{G}_i\left(\cdot\right)$ represents the weight of the $i^{th}$ trained LoRA. So, the final output of this block is computed by adding the output of the gating function to the output of the pre-trained network:
%
\begin{equation}
\boldsymbol{O}\left(\boldsymbol{x}\right) = \boldsymbol{F}_{\theta}\left(\boldsymbol{x}\right) + \Tilde{\boldsymbol{E}}_{\Omega}\left(\boldsymbol{x}\right).
\end{equation}
%
Besides, we conducted an exploration of \our's performance when employing gating functions at different hierarchical levels~(layer-wise and matrix-wise, etc). Please refer to Section~\ref{Sec:detailed_analysis}.

% \noindent\textbf{Gating Diversified LoRAs}

% \begin{figure}
% \centering
% \includegraphics[width=0.6\linewidth, height=0.6\linewidth]{gating_imbalance_compare.png}
% \hspace{-5mm}
% \includegraphics[width=0.39\linewidth, height=0.6\linewidth]{gating_imbalance_bar_compare.png}
% \\
% \vspace{-1mm}
% \makebox[0.6\linewidth]{\small (a)}
% \hspace{-2mm}
% \makebox[0.39\linewidth]{\footnotesize (b)}
% \vspace{-2mm}
% \caption{\textbf{Gating function with out $\mathcal{L}_{\text{balance}}$ loss tends to produce non-uniform distributions.} (a) The average gating entropy of all gating functions varies with the training steps. (b) The average weight distribution (\%) of three LoRAs w/ or w/o gating balance loss $\mathcal{L}_{\text{balance}}$. We observe that $\mathcal{L}_{\text{balance}}$ can mitigate the phenomenon of uneven predicted weight distribution in \our{} to a certain extent.}
% \label{fig:gating_imbalance_compare}
% \vspace{-3mm}
% \end{figure}



\subsection{Training Objective}
\label{Sec:training}
\noindent\textbf{Gating Balancing Loss}.
As shown in Figure~\ref{fig:gating_imbalance_compare}~(a), we observed that the average entropy of the distribution probabilities from the gating functions gradually decreases as the number of training steps increases,~i.e., the gating function tends to converge to a state where it always produces large weights for a early-stage well-performing LoRA~(e.g., shown in Figure.~\ref{fig:gating_imbalance_compare}~(b), 68\% gating probability for LoRA $\beta$ among three LoRAs), leading to only a handful of LoRAs having a significant impact in the end and a loss of the characteristics of other LoRAs.
%
%
%
\begin{wrapfigure}{R}{0.45\textwidth}
\vspace{-10mm}
\centering
\includegraphics[width=0.6\linewidth, height=0.6\linewidth]{gating_imbalance_compare.png}
\hspace{-5mm}
\includegraphics[width=0.39\linewidth, height=0.6\linewidth]{gating_imbalance_bar_compare.png}
\\
\vspace{-1mm}
\makebox[0.6\linewidth]{\small (a)}
\hspace{-5mm}
\makebox[0.39\linewidth]{\footnotesize (b)}
\vspace{-2mm}
\caption{(a) The average gating entropy of all gating functions varies with the training steps. (b) The average weight distribution (\%) of three LoRAs w and w/o $\mathcal{L}_{\text{balance}}$.}
\label{fig:gating_imbalance_compare}
\vspace{-14mm}
\end{wrapfigure}
%
%
%
To alleviate this, we propose a gating balancing loss $\mathcal{L}_{\text{balance}}$ as
\begin{equation}
 \mathcal{L}_{\text{balance}} = - \log\left(\prod_{i=0}^{N} \textbf{q}^{(i)}\right),
\end{equation}
where 
\begin{equation}
\textbf{q}^{(i)} = \frac{1}{M}\sum_{k=1}^M \frac{\exp\left(\varepsilon_i^k / \tau\right)}{\sum_{j=1}^{N} \exp\left(\varepsilon_j^k / \tau\right)},
\end{equation}
and $M$ represents the number of blocks where gating functions are placed and $N$ denotes the number of LoRAs. This balanced loss encourages balanced gating because it is minimized when the dispatching is ideally balanced.

\noindent\textbf{Domain-specific Loss}. 
Additionally, for adaptation to different domains, we employ distinct domain-specific training objectives denoted as $\mathcal{L}_{\text{D}}$.
%
In V\&L domain. we employ unsupervised training with both local and global guidance from CLIP~\citep{radford2021learning} to optimize \our{}.
%
In NLP domain, we follow the loss function in FLAN-T5~\citep{flant5}.
%
\begin{table*}[t]
\centering
\setlength{\tabcolsep}{6pt}
\caption{Text-alignment and image-alignment results for multiple LoRAs composition in CLIP feature space. NLA denotes normalized linear arithmetic composition (Eq.~\ref{Eq.Normalize-Combination}). The best performance is in bold.}
\vspace{-3mm}
\resizebox{\linewidth}{!}{
\begin{tabular}{cccc|ccc|ccc|ccc}
\toprule[1pt]
\# Visual Concepts & \multicolumn{3}{c}{Text-alignment} & \multicolumn{3}{c}{\shortstack[c]{Image-alignment, \\ (Concept 1)}} & \multicolumn{3}{c}{\shortstack[c]{Image-alignment, \\ (Concept 2)}} & \multicolumn{3}{c}{\shortstack[c]{Image-alignment, \\ (Concept 3)}} \\
 \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
 & NLA & SVDiff & \our & NLA & SVDiff & \our & NLA & SVDiff & \our & NLA & SVDiff & \our \\
\midrule
\small Fancy boot + Monster + Clock & 0.754 & 0.742 & 0.832%
& 0.781 & 0.758 & 0.784 %
& 0.791 & 0.749 & 0.801 %
& 0.763 & 0.812 & 0.809 \\

\small Emoji + Car + Cartoon & 0.610 & 0.607 & 0.696 %
& 0.619 & 0.734 & 0.839 %
& 0.711 & 0.702 & 0.709 %
& 0.652 & 0.686 & 0.679 \\

\small Vase + Wolf plushie + Teapot & 0.752 & 0.812 & 0.863 %
& 0.687 & 0.807 & 0.835 %
& 0.705 & 0.782 & 0.746 %
& 0.653 & 0.694 & 0.721 \\

\small White Cat + Wolf plushie + Can & 0.704 & 0.772 & 0.780 %
& 0.801 & 0.804 & 0.802 %
& 0.678 & 0.763 & 0.825 %
& 0.650 & 0.729 & 0.714 \\

\small Shiny sneaker + Wolf plushie + Teapot & 0.778 & 0.789 & 0.791 %
& 0.812 & 0.783 & 0.690 %
& 0.723 & 0.751 & 0.790 %
& 0.688 & 0.676 & 0.721 \\

\small Car + Wolf plushie + Teapot & 0.635 & 0.681 & 0.684 %
& 0.652 & 0.763 & 0.713 & %
0.601 & 0.664 & 0.745 %
& 0.685 & 0.612 & 0.707 \\

\small Can + Wolf plushie + backpack & 0.601 & 0.782 & 0.754 %
& 0.653 & 0.705 & 0.767 %
& 0.602 & 0.755 & 0.782 %
& 0.681 & 0.738 & 0.723 \\

\small Golden Retriever + Wolf plushie + Teapot & 0.670 & 0.716 & 0.784 
%
& 0.713 & 0.784 & 0.790 %
& 0.601 & 0.802 & 0.809 %
& 0.678 & 0.761 &  0.748 \\

\small Golden Retriever + Boot + Monster & 0.614 & 0.762 & 0.755%
& 0.665 & 0.662 & 0.620 %
& 0.748 & 0.832 & 0.862 %
& 0.723 & 0.719 & 0.735 \\

\small Backpack dog + Bowl + Teapot & 0.607 & 0.712 & 0.703
%
& 0.653 & 0.672 & 0.756 %
& 0.734 & 0.720 & 0.755 %
& 0.692 & 0.688 & 0.701 \\

\small Backpack dog + White Cat + Emoji & 0.648 & 0.703 & 0.717
%
& 0.674 & 0.692 & 0.812 %
& 0.719 & 0.741 & 0.701 %
& 0.742 & 0.720 & 0.796 \\

\small Dog + Wolf + Backpack & 0.717 & 0.738 & 0.722 
%
& 0.547 & 0.565 & 0.552 %
& 0.679 & 0.681 & 0.707 %
& 0.766 & 0.795 & 0.831\\

\small Cat + Sunglasses + Boot & 0.770 & 0.791 & 0.837 & 0.845 & 0.793 & 0.815 & 0.845 & 0.793 & 0.815 & 0.845 & 0.793 & 0.815 \\

\small Table + Can + Teapot & 0.836 & 0.827 & 0.810 & 0.753 & 0.770 & 0.741 & 0.751 & 0.799 & 0.806 & 0.818 & 0.771 & 0.829 \\

\small Robot + Dog + Clock & 0.663 & 0.638 & 0.693 & 0.689 & 0.764 & 0.797 & 0.645 & 0.674 & 0.710 & 0.661 & 0.715 & 0.717\\

\hc Average & 0.678 & 0.728 & \textbf{0.759}& 0.715 & 0.746 & \textbf{0.783} & 0.682 & 0.731 & \textbf{0.756} & 0.686 & 0.708 & \textbf{0.732} \\
\bottomrule[1pt]
\end{tabular}
}
\label{tb: VL_main_tab2}
\vspace{-3mm}
\end{table*}
%
The overall training objective $\mathcal{L}$ is the weighted sum of the above-mentioned two losses, represented as:
\begin{equation}
   \mathcal{L} = \mathcal{L}_{\text{D}} + \alpha\mathcal{L}_{\text{balance}},
\end{equation}
%
where $\alpha$ is a coefficient for weight balancing.
%

\noindent\textbf{Optimization Gating Function Only.}
We freeze all trained LoRAs and pre-trained model parameters, optimizing only the gating function's parameters. This helps preserve characteristics of trained LoRAs, particularly when training data is limited.

\section{Experiments}
\label{sec:experiment}
%
\subsection{\our{} on V\&L domain}
\noindent\textbf{Experimental Setup.} For V\&L domain, we apply \our{} to multi-subjects text-to-image generation task and choose DreamBooth~\citep{ruiz2023dreambooth}~(built on Stable Diffusion V2.1) as the base generator. 
%
Following the common setting~\citep{han2023svdiff, TI}, where 2 to 3 concepts are typically composed into a new multi-concept image, we conduct experiments by composing three separate trained LoRAs.
%
During training \our, we process the image resolution to 512$\times$512 and set learning rate as 1e-5. We use DDPM sampler~\citep{DDPM} with 50 steps in each case and train 400 iterations for each required composition with batch size 2 and $\alpha$ as 0.5.
%

\noindent\textbf{Metrics and Compared Baselines.} Following~\citep{ruiz2023dreambooth, han2023svdiff}, we evaluate our method on (1) Image alignment. The visual similarity of generated images with the individual composed concepts, using similarity in CLIP~\citep{clip} image feature space, (2) Text-alignment of the generated images with given text prompts, using text-image similarity in CLIP feature space~\citep{clip}. For each composition, we calculated the average scores among 200 generated images per prompt using 5 text prompts.
%
We compared our \our{} with normalized linear arithmetic composition (Eq.~\ref{Eq.Normalize-Combination}) and SVDiff~\citep{han2023svdiff}. Additionally, to further validate the effectiveness of \our, we also compare \our{} with state-of-the-art multi-subjects generation methods (full-parameters training based), which can be found in Section~\ref{Sec:detailed_analysis}.
%
\begin{table*}[t]
\centering
\setlength{\tabcolsep}{4pt}
\caption{Text-alignment and image-alignment results for multiple LoRA experts composition in CLIP feature space. The best performance is in bold and \underline{the second-best value} is indicated with an underline. NLA denotes normalized linear arithmetic composition (Eq.~\ref{Eq.Normalize-Combination}). \emph{SOTA full-parameter training methods are highlighted by} \textcolor{white}{\colorbox{purple!10}{\quad}}.}
\vspace{-3mm}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccccccc}
\toprule
\multirow{2}{*}{\shortstack{\# Number of Concepts}} & \multicolumn{5}{c}{Text-alignment} & \multicolumn{5}{c}{Average Image-alignment} \\
\cmidrule(lr){2-6} \cmidrule(lr){7-11}
\tiny & NLA & \cellcolor{purple!10} Custom & \cellcolor{purple!10} Textual Inversion & SVDiff & \our & NLA & \cellcolor{purple!10} Custom & \cellcolor{purple!10} Textual Inversion & SVDiff & \our \\
\midrule
3 & 0.678 & \underline{0.751} & 0.709 & 0.728 & \textbf{0.759} %
& 0.694 & \textbf{0.761} & 0.720 & 0.719 & \underline{0.757} \\

4 & 0.681 & \textbf{0.735} & 0.721 & 0.717 & \underline{0.725} %
& 0.712 & \textbf{0.760} & 0.736 & 0.721 & \underline{0.742} \\

5 & 0.652 & \underline{0.731} & 0.704 & 0.723 & \textbf{0.762} %
& 0.682 & \textbf{0.798} & 0.710 & 0.708 & \underline{0.737} \\

6 & 0.678 & 0.722 & \textbf{0.735} & 0.709 & \underline{0.727} %
& 0.698 & 0.721 & \textbf{0.747} & 0.712 & \underline{0.736} \\

\hc Average & 0.672 & \underline{0.734} & 0.717 & 0.719 & \textbf{0.752} & 0.692 & \textbf{0.760} & 0.728 & 0.715 & \underline{0.743} \\
\bottomrule
\end{tabular}
}
\label{tb: VL_compare_IT_custom}
\vspace{-5mm}
\end{table*}

\noindent\textbf{Main Results.} As shown in Table~\ref{tb: VL_main_tab2}, this study involves 15 different compositions of three visual subjects.
%
%
%
The overall results show that our method significantly outperforms other comparative methods in terms of Text-alignment score, with a 0.031 average improvement compared to SVDiff, as well as the Image-alignment score associated with three visual concepts~(e.g., 0.037 average improvement compared to SVDiff in concept 1), providing evidence of of our \our's superior capability in accurately capturing and depicting the subject information of user-provided images, as well as displaying multiple entities concurrently within a single image.
%
Significantly, prior research~\citep{kumari2023multi,gal2022image} indicates a trade-off between Text-alignment and Image-alignment scores in multi-subjects generation. Excelling in both scores is challenging, highlighting the strength of our \our{}.
%
% \begin{table}[t]
% \centering
% \setlength{\tabcolsep}{2pt}
% \caption{Evaluation results on Translation, Struct to Text, Closed-Book QA, NLI and BBH. The \textbf{best value} is in bold and \underline{the second-best value} is underlined.}
% \vspace{-2mm}
% \label{tab:NLP_Main}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{lcccc}
% \toprule
% \# \large Task & Metric & LoRAHub & PEMs & \our\\
% \midrule
% \underline{\textbf{Translation}} \\
% WMT '14 En$\rightarrow$Fr & BLEU & \underline{27.4} & 25.6 & \textbf{29.1} \\
% WMT '14 Fr$\rightarrow$En & BLEU & \underline{29.4} & 27.1 & \textbf{31.3} \\
% WMT '16 En$\rightarrow$De & BLEU & 24.6 & \underline{24.9} & \textbf{27.7} \\
% WMT '16 De$\rightarrow$En & BLEU & \textbf{29.9} & 28.0 & \underline{29.1} \\
% WMT '16 En$\rightarrow$Ro & BLEU & \underline{17.7} & 15.2 & \textbf{18.9} \\
% WMT '16 Ro$\rightarrow$En & BLEU & \underline{23.5} & 21.7 & \textbf{25.1} \\
% \hc Average &  & \underline{25.4} & 24.2 & \textbf{26.9}\\
% \midrule 
% \underline{\textbf{Struct to Text}} \\
% CommonGen & Rouge-1 & \underline{53.7} & 48.8 & \textbf{55.1} \\
% & Rouge-2 & \textbf{23.1} & 22.4 & \underline{23.1} \\
% & Rouge-L & \underline{49.7} & 47.2 & \textbf{53.9} \\
% DART & Rouge-1 & 45.3 & \underline{46.2} & \textbf{48.8} \\
% & Rouge-2 & \underline{22.6} & 18.9 & \textbf{23.5} \\
% & Rouge-L & 35.1 & \textbf{37.6} & \underline{36.0} \\
% E2ENLG & Rouge-1 & \underline{41.1} & 40.7 & \textbf{42.0} \\
% & Rouge-2 & \underline{26.3} & 24.2 & \textbf{29.0} \\
% & Rouge-L & 38.8 & \textbf{42.1} & \underline{41.8} \\
% WebNLG & Rouge-1 & \underline{52.1} & 52.0 & \textbf{54.5} \\
% & Rouge-2 & 23.9 & \underline{24.6} & \textbf{26.8} \\
% & Rouge-L & 45.2 & \underline{47.8} & \textbf{49.3} \\
% \hc Average & & \underline{38.1} & 37.7 & \textbf{40.3} \\
% \midrule
% \underline{\textbf{Closed-Book QA}} \\
% ARC-c & EM & \underline{51.7} & 50.4 & \textbf{52.9} \\
% ARC-e & EM& \underline{69.7} & 65.7 & \textbf{70.3} \\
% NQ & EM& \underline{17.3} & 16.1 & \textbf{23.5} \\
% TQA & EM & \textbf{54.5} & 53.9 & \underline{54.0} \\
% \hc Average & & \underline{48.3} & 46.5 & \textbf{50.2} \\
% \midrule 
% \underline{\textbf{Big-Bench Hard~(BBH)}} \\
% Boolean Expressions & EM & \underline{55.1} & 53.0 & \textbf{57.3} \\
% Causal Judgement & EM & \underline{57.6} & 51.1 & \textbf{57.9} \\
% Date Understanding & EM & \textbf{31.0} & 29.3 & \underline{30.7} \\
% Disambiguation & EM & 46.6 & \underline{47.2} & \textbf{49.3} \\
% Penguins in a Table & EM & \underline{41.4} & 39.8 & \textbf{45.0} \\
% Reasoning Objects & EM & \underline{35.2} & \textbf{37.5} & 33.7 \\
% Ruin Names & EM & \underline{19.9} & 19.3 & \textbf{21.2} \\
% \hc Average &  & \underline{38.4} & 33.2 & \textbf{42.2} \\
% \midrule 
% \underline{\textbf{Natural Language Inference~(NLI)}} \\
% ANLI-R1 & EM & \underline{81.0} &  80.3 & \textbf{82.7} \\
% ANLI-R2 & EM & \underline{80.9} & 80.2 & \textbf{82.4} \\
% ANLI-R3 & EM & \underline{77.4} & 76.6 & \textbf{78.9} \\
% QNLI & EM & 77.6 & \underline{78.0} & \textbf{78.1}\\
% \hc Average &  & \underline{79.2} & 78.8 & \textbf{80.5}\\
% \bottomrule
% \end{tabular}
% }
% \vspace{-2mm}
% \end{table}
%
%
Additionally, as shown in Figure~\ref{fig:VL_main_pic},~\ref{fig:VL_main_pic2} and~\ref{fig:VL_main_pic3}, our approach outperforms two other methods in preserving subject fidelity in generated images. The comparative methods often omit a subject, as seen in the NLA composition's failure to include elements like ``cat'' in Figure~\ref{fig:VL_main_pic} (line 2) and ``barn'' in Figure~\ref{fig:VL_main_pic2}, and SVDiff's inability to precisely represent ``dog'' and ``cat'' in Figure~\ref{fig:VL_main_pic2}. Furthermore, while these methods can generate images with three subjects, there's a noticeable leakage and mixing of appearance features, resulting in lower subject fidelity compared to user-provided images. In contrast, our method effectively retains the subjects specified by the user, with each accurately depicted.

%
\begin{wrapfigure}{R}{0.55\textwidth}
\vspace{-4mm}
\centering
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{3pt}
\begin{tabular}{lcccc}
\toprule
\# Task & Metric & LoRAHub & PEMs & \our\\
\midrule
\underline{\textbf{Translation}} \\
WMT '14 En$\rightarrow$Fr & BLEU & \underline{27.4} & 25.6 & \textbf{29.1} \\
WMT '14 Fr$\rightarrow$En & BLEU & \underline{29.4} & 27.1 & \textbf{31.3} \\
WMT '16 En$\rightarrow$De & BLEU & 24.6 & \underline{24.9} & \textbf{27.7} \\
WMT '16 De$\rightarrow$En & BLEU & \textbf{29.9} & 28.0 & \underline{29.1} \\
WMT '16 En$\rightarrow$Ro & BLEU & \underline{17.7} & 15.2 & \textbf{18.9} \\
WMT '16 Ro$\rightarrow$En & BLEU & \underline{23.5} & 21.7 & \textbf{25.1} \\
\hc Average &  & \underline{25.4} & 24.2 & \textbf{26.9}\\
\midrule 
\underline{\textbf{Struct to Text}} \\
CommonGen & Rouge-1 & \underline{53.7} & 48.8 & \textbf{55.1} \\
& Rouge-2 & \textbf{23.1} & 22.4 & \underline{23.1} \\
& Rouge-L & \underline{49.7} & 47.2 & \textbf{53.9} \\
DART & Rouge-1 & 45.3 & \underline{46.2} & \textbf{48.8} \\
& Rouge-2 & \underline{22.6} & 18.9 & \textbf{23.5} \\
& Rouge-L & 35.1 & \textbf{37.6} & \underline{36.0} \\
E2ENLG & Rouge-1 & \underline{41.1} & 40.7 & \textbf{42.0} \\
& Rouge-2 & \underline{26.3} & 24.2 & \textbf{29.0} \\
& Rouge-L & 38.8 & \textbf{42.1} & \underline{41.8} \\
WebNLG & Rouge-1 & \underline{52.1} & 52.0 & \textbf{54.5} \\
& Rouge-2 & 23.9 & \underline{24.6} & \textbf{26.8} \\
& Rouge-L & 45.2 & \underline{47.8} & \textbf{49.3} \\
\hc Average & & \underline{38.1} & 37.7 & \textbf{40.3} \\
\midrule
\underline{\textbf{Closed-Book QA}} \\
ARC-c & EM & \underline{51.7} & 50.4 & \textbf{52.9} \\
ARC-e & EM& \underline{69.7} & 65.7 & \textbf{70.3} \\
NQ & EM& \underline{17.3} & 16.1 & \textbf{23.5} \\
TQA & EM & \textbf{54.5} & 53.9 & \underline{54.0} \\
\hc Average & & \underline{48.3} & 46.5 & \textbf{50.2} \\
\midrule 
\underline{\textbf{Big-Bench Hard~(BBH)}} \\
Boolean Expressions & EM & \underline{55.1} & 53.0 & \textbf{57.3} \\
Causal Judgement & EM & \underline{57.6} & 51.1 & \textbf{57.9} \\
Date Understanding & EM & \textbf{31.0} & 29.3 & \underline{30.7} \\
Disambiguation & EM & 46.6 & \underline{47.2} & \textbf{49.3} \\
Penguins in a Table & EM & \underline{41.4} & 39.8 & \textbf{45.0} \\
Reasoning Objects & EM & \underline{35.2} & \textbf{37.5} & 33.7 \\
Ruin Names & EM & \underline{19.9} & 19.3 & \textbf{21.2} \\
\hc Average &  & \underline{38.4} & 33.2 & \textbf{42.2} \\
\midrule 
\underline{\textbf{Natural Language Inference~(NLI)}} \\
ANLI-R1 & EM & \underline{81.0} &  80.3 & \textbf{82.7} \\
ANLI-R2 & EM & \underline{80.9} & 80.2 & \textbf{82.4} \\
ANLI-R3 & EM & \underline{77.4} & 76.6 & \textbf{78.9} \\
QNLI & EM & 77.6 & \underline{78.0} & \textbf{78.1}\\
\hc Average &  & \underline{79.2} & 78.8 & \textbf{80.5}\\
\bottomrule
\end{tabular}
}
\vspace{-2mm}
\captionof{table}{Evaluation results on Translation, Struct to Text, Closed-Book QA, NLI and BBH. The \textbf{best value} is in bold and \underline{the second-best value} is underlined.}
\label{tab:NLP_Main}
\vspace{-15mm}
\end{wrapfigure}
%
%
\subsection{\our{} on NLP domain}
\noindent\textbf{Experimental Setup.} For NLP domain,~following~\citep{huang2023lorahub}, we employ Flan-T5~\citep{flant5} as our chosen LLM and created several LoRAs based on FLAN datasets. 
%
We conducted extensive experiments across various tasks, including Translation, Natural Language Inference~(NLI), Struct to Text, Closed-Book QA, and multiple subtasks within the Big-Bench Hard (BBH)~\citep{ghazal2013bigbench} dataset.
%
We train 800 iterations for each required composition of LoRAs with an initial learning rate of 1e-5, batch size 12 and $\alpha$ as 0.5.
%

\noindent\textbf{Compared Baselines.} We compared our \our{} with recently released state-of-the-art LoRA composition methods: LoRAhub and PEMs.

\noindent\textbf{Main Results.} The corresponding experimental results are encapsulated in the Table~\ref{tab:NLP_Main}. In summary, our \our{} surpasses state-of-the-art LoRA composition methods on five distinct datasets. Notably, on the BBH dataset, our \our{} achieves an average performance improvement of 3.8 over LoRAHub and outperforms PEMs by a notable margin of 9.0.
%
Furthermore, in the realm of generation tasks, specifically in Translation and Struct to Text categories, \our{} consistently outshines its counterparts. In the Translation task set, it surpasses LoRAHub by an average margin of 1.5 and PEMs by 2.7. Correspondingly, within the Struct to Text task set, our model boasts an average performance superiority of 2.1 over LoRAHub and 2.6 over PEMs. These findings underscore the efficacy and versatility of our \our{} in handling language generation tasks.

\section{Analysis}
\label{Sec:detailed_analysis}
%
\vspace{-1mm}
\noindent\textbf{The effectiveness of gating balancing loss}. Figure~\ref{fig:gating_imbalance_compare} (a) and (b) illustrate how our $\mathcal{L}_{\text{balance}}$ function mitigates the reduction in entropy rates within gating functions, leading to a more uniform composition weight distribution.
%
The performance comparison between \our{} and \our{} $_{w/o~\mathcal{L}_{\text{balance}}}$ in Table~\ref{tb: NLP_tempare} underscores the performance enhancement achieved with the inclusion of $\mathcal{L}_{\text{balance}}$.
%
%
Additionally, we conducted an experiment wherein we solely increased the temperature $\tau$ in Eq.~\ref{EQ. temp}, as an alternative to adding $\mathcal{L}_{\text{balance}}$. Results in Table~\ref{tb: NLP_tempare} shows declining performance in \our{} variants \our$^{\tau_1}$, \our$^{\tau_2}$, \our$^{\tau_3}$ ($\tau_1 \prec \tau_2 \prec \tau_3$) with increasing temperature. While temperature rise addresses gating imbalance, it restricts dynamic LoRA exploration in \our, leading to inferior outcomes.

\noindent\textbf{Further comparison with SOTA multi-concept generation methods}. In the absence of comparable LoRA composition methods in the V\&L domain, we incorporated two leading multi-concept generation algorithms that do not utilize LoRA: Custom~\citep{kumari2023multi} and Textual Inversion~\citep{TI}, both of which emphasize full-parameter training for enhanced results.
% %
% \begin{wrapfigure}{R}{0.55\textwidth}
% % \vspace{-2mm}
% \centering
% \resizebox{\linewidth}{!}{
% \setlength{\tabcolsep}{3pt}
% \begin{tabular}{lcccc}
% \toprule
% \# \large Task & Metric & LoRAHub & PEMs & \our\\
% \midrule
% \underline{\textbf{Translation}} \\
% WMT '14 En$\rightarrow$Fr & BLEU & \underline{27.4} & 25.6 & \textbf{29.1} \\
% WMT '14 Fr$\rightarrow$En & BLEU & \underline{29.4} & 27.1 & \textbf{31.3} \\
% WMT '16 En$\rightarrow$De & BLEU & 24.6 & \underline{24.9} & \textbf{27.7} \\
% WMT '16 De$\rightarrow$En & BLEU & \textbf{29.9} & 28.0 & \underline{29.1} \\
% WMT '16 En$\rightarrow$Ro & BLEU & \underline{17.7} & 15.2 & \textbf{18.9} \\
% WMT '16 Ro$\rightarrow$En & BLEU & \underline{23.5} & 21.7 & \textbf{25.1} \\
% \hc Average &  & \underline{25.4} & 24.2 & \textbf{26.9}\\
% \midrule 
% \underline{\textbf{Struct to Text}} \\
% CommonGen & Rouge-1 & \underline{53.7} & 48.8 & \textbf{55.1} \\
% & Rouge-2 & \textbf{23.1} & 22.4 & \underline{23.1} \\
% & Rouge-L & \underline{49.7} & 47.2 & \textbf{53.9} \\
% DART & Rouge-1 & 45.3 & \underline{46.2} & \textbf{48.8} \\
% & Rouge-2 & \underline{22.6} & 18.9 & \textbf{23.5} \\
% & Rouge-L & 35.1 & \textbf{37.6} & \underline{36.0} \\
% E2ENLG & Rouge-1 & \underline{41.1} & 40.7 & \textbf{42.0} \\
% & Rouge-2 & \underline{26.3} & 24.2 & \textbf{29.0} \\
% & Rouge-L & 38.8 & \textbf{42.1} & \underline{41.8} \\
% WebNLG & Rouge-1 & \underline{52.1} & 52.0 & \textbf{54.5} \\
% & Rouge-2 & 23.9 & \underline{24.6} & \textbf{26.8} \\
% & Rouge-L & 45.2 & \underline{47.8} & \textbf{49.3} \\
% \hc Average & & \underline{38.1} & 37.7 & \textbf{40.3} \\
% \midrule
% \underline{\textbf{Closed-Book QA}} \\
% ARC-c & EM & \underline{51.7} & 50.4 & \textbf{52.9} \\
% ARC-e & EM& \underline{69.7} & 65.7 & \textbf{70.3} \\
% NQ & EM& \underline{17.3} & 16.1 & \textbf{23.5} \\
% TQA & EM & \textbf{54.5} & 53.9 & \underline{54.0} \\
% \hc Average & & \underline{48.3} & 46.5 & \textbf{50.2} \\
% \midrule 
% \underline{\textbf{Big-Bench Hard~(BBH)}} \\
% Boolean Expressions & EM & \underline{55.1} & 53.0 & \textbf{57.3} \\
% Causal Judgement & EM & \underline{57.6} & 51.1 & \textbf{57.9} \\
% Date Understanding & EM & \textbf{31.0} & 29.3 & \underline{30.7} \\
% Disambiguation & EM & 46.6 & \underline{47.2} & \textbf{49.3} \\
% Penguins in a Table & EM & \underline{41.4} & 39.8 & \textbf{45.0} \\
% Reasoning Objects & EM & \underline{35.2} & \textbf{37.5} & 33.7 \\
% Ruin Names & EM & \underline{19.9} & 19.3 & \textbf{21.2} \\
% \hc Average &  & \underline{38.4} & 33.2 & \textbf{42.2} \\
% \midrule 
% \underline{\textbf{Natural Language Inference~(NLI)}} \\
% ANLI-R1 & EM & \underline{81.0} &  80.3 & \textbf{82.7} \\
% ANLI-R2 & EM & \underline{80.9} & 80.2 & \textbf{82.4} \\
% ANLI-R3 & EM & \underline{77.4} & 76.6 & \textbf{78.9} \\
% QNLI & EM & 77.6 & \underline{78.0} & \textbf{78.1}\\
% \hc Average &  & \underline{79.2} & 78.8 & \textbf{80.5}\\
% \bottomrule
% \end{tabular}
% }
% \vspace{-2mm}
% \captionof{table}{Evaluation results on Translation, Struct to Text, Closed-Book QA, NLI and BBH. The \textbf{best value} is in bold and \underline{the second-best value} is underlined.}
% \label{tab:NLP_Main}
% \vspace{-4mm}
% \end{wrapfigure}
% %
As presented in Table~\ref{tb: VL_compare_IT_custom}, \our{} outperforms Textual Inversion in both image and text alignment and excels over Custom in text alignment. Furthermore, it's worth noting that our MoLE is more lightweight compared to these full-parameter training methods. These comparisons underscore the superior effectiveness of our MoLE relative to methods that involve extensive parameter tuning.

\noindent\textbf{Scale to a larger number of LoRAs}. We explore the performance as the number of LoRAs increases. 
%
In the NLP domain, experiments were conducted with varying numbers of LoRA (8, 24, 48, 128), as detailed in Table~\ref{tb: NLP_large_number_lora}. Our \our{} demonstrated optimal performance across these configurations, notably excelling with larger LoRA counts of 48 and 128, surpassing LoRAHub by \textbf{2.5} and \textbf{3.0}, respectively. Analysis revealed that LoRAHub's optimization algorithm often zeroes out many LoRA weights in larger arrays, thus underutilizing the potential of all LoRA. Conversely, \our{} effectively overcomes this limitation. However, all methods, including \our, showed performance declines with an extremely large number of LoRA (128), highlighting a need for further research in this area.
%
In the V\&L domain, Table~\ref{tb: VL_large_lora} shows experiments with increased composed LoRAs. While typical composition involve 3-4 visual concepts, our range was 3-6 to avoid ambiguity in outputs. Results indicate that \our{} consistently outperforms other LoRA composition models in text and image alignment as the number of LoRAs increases, underscoring its robustness and superior composition capabilities.

\noindent\textbf{Coarse-to-fine gating analysis}. To examine the impact of different granularity levels in gating functions, we delineated four levels in \our: matrix-wise (\our, gating at the parameter matrix level), layer-wise (\our), block-wise (\our), and network-wise (\our), abbreviated as m-\our, l-\our, b-\our, and n-\our{} respectively.
%
Table~\ref{tb: c-to-f MOLE} reveals that intermediate granularities, b-\our{} and l-\our, achieved the highest performance. In contrast, the coarsest level, n-\our, which involves minimal optimizable parameters (a single gating for the entire network), showed suboptimal outcomes.
%
Additionally, the finest granularity, m-\our, underperformed, potentially due to its excessive control interfering with inherent relationships in LoRA parameters.

\noindent\textbf{Generalization to new datasets}. To further validate the effectiveness of our \our, we conducted generalization experiments. Specifically, all LoRA candidates and LoRA composition variants, including \our, PEMs and LoRAHub, were trained on NLI tasks~(ANLI-R1, ANLI-R2, ANLI-R3, QNLI, and WNLI, among others). Subsequently, we evaluated these methods on the BBH dataset.
%
As illustrated in Table~\ref{tab:NPL_generalization}, our \our{} achieves an average performance advantage of 2.4 over LoRAHub and 3.7 over PEMs, underscoring its superior generalization ability.

\noindent\textbf{Flexibility of \our}. As discussed in Section~\ref{Sec:LoRA Merging}, a well-designed LoRA composition method should not only achieve effective LoRA composition but also retain the characteristics of individual LoRA. It should be versatile enough to function as a standalone LoRA generator, ensuring its practical applications are flexible and widespread.
%
Figure~\ref{fig:retain_ability} displays a comparison of the qualitative results for the retaining ability of several composition methods, we find that our \our{} can generate images that closely resemble the original features of the LoRA experts (e.g., dog ears, the color of the backpack), while other composition methods tend to produce confusion and loss of LoRA characteristics.
%
Besides, as shown in Figure~\ref{fig:workflow}, we can also degrade \our{} by masking out the LoRA experts we do not wish to use, transforming it into a \our{} that merges fewer LoRAs without affecting the composition effect of the remaining LoRAs. As shown in Figure~\ref{fig:retain_ability2}, our \our{} can achieve the same flexible LoRA composition as linear arithmetic composition method without altering the weights of \our{}, while reference tuning-based composition~\citep{gu2023mix} can not accomplish.

\noindent\textbf{Hierarchical control analysis}. \our{} aims to achieve improved LoRA composition effects through finer-grained hierarchical control. As illustrated in the Figure~\ref{fig:gating_div_vis}, we visualize the weight distributions assigned by the gating functions learned by \our{} at different levels in both NLP and V\&L domains. 
%
We observe that \our{} adaptively assigns weights to different LoRA experts at various layers. Consequently, finer-grained weight combination methods lead to superior results.
%

\vspace{-1mm}

\section{Conclusion and Limitations}
\vspace{-2mm}
In this study, we introduce the Mixture of LoRA Experts (\our) as a versatile and dynamic approach for composing multiple trained LoRAs. The key innovation of \our{} lies in its learnable gating functions, which utilize the outputs of multiple LoRAs at each layer to determine composition weights. Our comprehensive evaluation in both the both NLP and V\&L domains establishes that \our{} outperforms existing LoRA composition methods.

\noindent\textbf{Limitations}. As described in Section~\ref{Sec:detailed_analysis}, when the number of LoRAs increases to a very large value (e.g., 128), despite our \our~exhibiting superior performance, the performance of all LoRA composition methods, including our \our, tends to decrease. This suggests that our \our~still faces challenges when performing large-scale LoRA composition. It also highlights the significance of researching better approaches for handling large-scale LoRA composition effectively.

% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}
\clearpage
\appendix
% \section{Domain-specific Loss}
% \label{Supp: loss}
% CLIP~\citep{radford2021learning} was introduced as an efficient framework for learning visual concepts with natural language supervision. In CLIP, a text encoder and an image encoder are pre-trained to establish associations between textual descriptions and corresponding images in the dataset.

% For our text-driven multi-concept generation task, we make use of a pre-trained CLIP model to effectively leverage its knowledge for optimizing our \our. To extract valuable guidance information from CLIP, two distinct loss functions have been proposed:~a global supervision loss~\citep{patashnik2021styleclip}, and a local directional loss~\citep{gal2021stylegan}.

% \noindent\textbf{Global supervision loss.} The global supervision loss aims to minimize the cosine distance in the CLIP feature space between the generated image and a specified target text, as expressed by the following equation:
% \begin{equation}
%     \mathcal{L}_{\text{global}} = \mathcal{D}_{\text{CLIP}}(\xb_\text{gen}, {y_\text{tar}}),
% \end{equation}
% In the above equations, $y_\text{tar}$ represents the target text prompt, $\xb_\text{gen}$ stands for the generated multi-concept image, and $\mathcal{D}_\text{CLIP}$ computes the cosine distance in the CLIP feature space between their respective encoded vectors.

% \noindent\textbf{Local directional loss.} In contrast, the local directional loss~\citep{gal2021stylegan} has been introduced to address the limitations of the global CLIP loss, such as limited diversity and vulnerability to adversarial attacks. The local directional CLIP loss enforces the alignment of the direction between the embeddings of the reference and generated images with the direction between the embeddings of a pair of reference and target prompts in the CLIP feature space, as defined below:
%  \begin{eqnarray}
%      \mathcal{L}_{\text{direction}} = 1 - \sum^{N}_{i=0}\frac{\langle \Delta I_i, \Delta T_i\rangle}{\|\Delta I_i\|\|\Delta T_i\|},
% \end{eqnarray}
% where 
% \begin{equation}
% \Delta T_i =  \mathcal{E}_T{(y_{\text{tar}}}) - \mathcal{E}_T({y_{\text{ref}}}_i)
% \end{equation}
% and
% \begin{equation}
% \Delta I_i =  \mathcal{E}_I(\xb_{\text{gen}}) - \mathcal{E}_I(\xb_{\text{ref}}_i).
% \end{equation}
% Here $N$ denotes the number of composed concepts. $\mathcal{E}_I$ and $\mathcal{E}_T$ represent CLIP's image and text encoders, respectively, and ${y_{\text{ref}}_i}$ and $\xb_{\text{ref}}_i$ denote text and image for $i^{th}$ concept in the source domain, respectively. The application of the directional CLIP loss to guide image manipulation has been shown to mitigate mode-collapse issues. This is achieved by aligning the direction between the image representations with the direction between the reference text and the target text, resulting in the generation of distinct images. So the final $\mathcal{L}_{D}$ in V\&L domain is:
% \begin{equation}
% \mathcal{L}_{D} =  \mathcal{L}_{\text{global}} + \mathcal{L}_{\text{direction}}.
% \end{equation}

 
% \section{Trade-off Analysis in V\&L Domain}
% \label{Supp: trade-off}

% As shown in Figure.~\ref{fig:retain_ability2}, there is usually a trade-off between text-alignment and image-alignment~\citep{kumari2023multi,han2023svdiff}. High image-alignment leads to a decrease in text-alignment where sample generations have less variance and are close to input target images. As a result, our \our{} may not perform better in both image and text sub alignment. However, as shown in Table.\ref{tb: VL_main_tab2} it is noteworthy that when considering the overall average across all settings, our \our{} excels in both average text alignment and image alignment, which demonstrates that our \our{} is capable of achieving superior alignment between text and images.

% \begin{figure}[!h]
% \centering
% \includegraphics[width=0.4\linewidth]{dreambooth_tradeoff.png} %
% % \hfill
% \includegraphics[width=0.4\linewidth]{custom_tradeoff.png} %
% \\
% \centering
% \makebox[0.47\linewidth]{\footnotesize (a) DreamBooth}
% \makebox[0.47\linewidth]{\footnotesize (b) SVDiff} \\
% \makebox[0.47\linewidth]{\footnotesize~\citep{ruiz2023dreambooth}} %
% \makebox[0.47\linewidth]{\footnotesize~\citep{han2023svdiff}} % 
% \caption{Under the V\&L experimental setup, There exists a trade-off between high similarity to target images vs. text-alignment.}
% \label{fig:retain_ability2}
% % \vspace{-4mm}
% \end{figure}

% \section{More Detailed Analysis of \our}
% \label{Supp: analysis}
% \noindent\textbf{Flexibility of \our}. As discussed in Section~\ref{Sec:LoRA Merging}, a well-designed LoRA composition method should not only achieve effective LoRA composition but also retain the characteristics of individual LoRA. It should be versatile enough to function as a standalone LoRA generator, ensuring its practical applications are flexible and widespread.
% %
% Figure~\ref{fig:retain_ability} displays a comparison of the qualitative results for the retaining ability of several composition methods, we find that our \our{} can generate images that closely resemble the original features of the LoRA experts (e.g., dog ears, the color of the backpack), while other composition methods tend to produce confusion and loss of LoRA characteristics.
% %
% Besides, as shown in Figure~\ref{fig:workflow}, we can also degrade \our{} by masking out the LoRA experts we do not wish to use, transforming it into a \our{} that merges fewer LoRAs without affecting the composition effect of the remaining LoRAs. As shown in Figure~\ref{fig:retain_ability2}, our \our{} can achieve the same flexible LoRA composition as linear arithmetic composition method without altering the weights of \our, while reference tuning-based composition~\citep{gu2023mix} can not accomplish.
%  \\
% \noindent\textbf{Hierarchical control analysis}. \our{} aims to achieve improved LoRA composition effects through finer-grained hierarchical control. As illustrated in the Figure~\ref{fig:gating_div_vis}, we visualize the weight distributions assigned by the gating functions learned by \our{} at different levels in both NLP and V\&L domains. 
% %
% We observe that \our{} adaptively assigns weights to different LoRA experts at various layers. Consequently, finer-grained weight combination methods lead to superior results.
% %
\clearpage

\begin{table*}
\centering
% \setlength{\tabcolsep}{6pt}
\caption{The first motivation experiment in the NLP domain. NLA denotes normalized linear arithmetic composition (Eq.~\ref{Eq.Normalize-Combination}). The \textbf{best value} is in bold.}
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{ccccccc}
\toprule
\bf Model & ANLI-R1	& ANLI-R2 &	ANLI-R3 & QNLI & WNLI & Average \\
\midrule
Single LoRA & \bf 80.32 & \bf 79.02 & 75.92 & \bf 78.62 & \bf 74.32 & \bf 77.64 \\
NLA & 79.32  & 78.88  & \bf 76.42 & 78.06  & 69.98  & 76.53 \\ 
\bottomrule
\end{tabular}
}
\label{tb: NLP_motivation1}
\end{table*}


\begin{table*}
\centering
\setlength{\tabcolsep}{12pt}
\caption{The second motivation experiment in the NLP domain. Full LoRA denotes the application of the complete set of LoRA parameters for inference, whereas x\%-y\% indicates the inference using LoRA parameters ranging from the top x\% to the top y\%. The \textbf{best value} is in bold.}
\resizebox{0.5\linewidth}{!}{
\begin{tabular}{cccc}
\toprule
& ANLI-R1 & ANLI-R2 & QNLI \\
\midrule
Full LoRA & 81.65	& 80.03	& 76.42\\
0\%-20\%	&78.72	&78.35	& \bf 78.14\\
20\%-40\%	&76.10	&77.96	&77.85\\
40\%-60\%	&76.95	&\bf 81.47	&74.57\\
60\%-80\%	&77.25	&78.19	&75.71\\
80\%-100\%&	\bf 82.59	&77.91	&75.48\\
\bottomrule
\end{tabular}
}
\label{tb: NLP_motivation2}
\end{table*}

\begin{table*}
\centering
\setlength{\tabcolsep}{6pt}
\caption{NLP domain experimental results on the impact of exploring expand expert numbers on model performance. The result is the average EM on the Big-Bench Hard (BBH) dataset. NLA denotes normalized linear arithmetic composition (Eq.~\ref{Eq.Normalize-Combination}). The \textbf{best value} is in bold and \underline{the second-best value} is indicated with an underline.}
\resizebox{0.5\linewidth}{!}{
\begin{tabular}{ccccc}
\toprule
\# Number of LoRA & NLA & LoRAHub & PEMs & \our \\
\midrule
8 & 32.7 & \underline{33.9} & 33.7 & \textbf{36.6} \\
24 & 36.8 & \underline{37.1} & 36.9 & \textbf{38.7} \\
48 & 34.4 & \underline{36.9} & 34.6 & \textbf{39.4} \\
128 & 34.1 & \underline{35.5} & 34.9 & \textbf{38.5} \\
\hc Average & 34.5 & \underline{35.9} & 35.0 & \textbf{38.3} \\ 
\bottomrule
\end{tabular}
}
\label{tb: NLP_large_number_lora}
\end{table*}

\begin{table*}
\centering
\setlength{\tabcolsep}{5pt}
\caption{Experimental results on gating balance of \our. NLA denotes normalized linear arithmetic composition (Eq.~\ref{Eq.Normalize-Combination}). The \textbf{best value} is in bold.}
\resizebox{0.6\linewidth}{!}{
\begin{tabular}{lcccccc}
\toprule
\# Model & ANLI-R1 & ANLI-R2 & ANLI-R3 & QNLI & WNLI & Average \\
\midrule
NLA & 79.32 & 78.88 & 76.42 & 78.06 & 69.98 & 76.53 \\
\hc \our{}   & \textbf{81.49}   & \textbf{79.38}   & \textbf{77.63}   & \textbf{79.52}   & \textbf{72.31}   & \textbf{78.07}   \\
\hb \our{} w/o $\mathcal{L}_{\text{balance}}$  &  80.81 & 79.11 & 77.42 & 79.09 & 71.44 & 77.57  \\
\ha \our$^{\tau_1}$    & 80.52   & 79.27   & 77.30   & 79.11   & 71.07   & 77.45   \\
\our$^{\tau_2}$      & 80.01   & 79.03   & 76.33   & 77.81   & 70.37   & 76.71   \\
\our$^{\tau_3}$      & 78.50   & 79.20   & 76.07   & 78.02   & 70.00   & 76.35   \\
\bottomrule
\end{tabular}
}
\label{tb: NLP_tempare}
\end{table*}

\begin{table*}[ht]
\centering
\caption{Evaluation results on generalization to new datasets. All lora candidates and LoRA merging variants are optimized on NLI tasks. The \textbf{best value} is in bold and \underline{the second-best value} is indicated with an underline.}
\resizebox{0.55\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
\# Task & Metric & LoRAHub & PEMs & \our\\
\midrule
\underline{\textbf{Big-Bench Hard~(BBH)}} \\
Boolean Expressions & EM & 45.3 & \underline{45.5} & \textbf{48.7} \\
Causal Judgement & EM & \underline{51.3} & 46.1 & \textbf{52.4} \\
Date Understanding & EM & \textbf{27.5} & 24.6 & \underline{26.6} \\
Disambiguation & EM & 39.7 & \underline{42.4} & \textbf{43.8} \\
Penguins in a Table & EM & \underline{35.3} & 33.6 &  \textbf{39.0} \\
Reasoning about Colored Objects & EM & \underline{32.2} & 31.4 & \textbf{34.7} \\
\hc Average &  & \underline{38.5} & 37.2 & \textbf{40.9} \\
\bottomrule
\end{tabular}}
\label{tab:NPL_generalization}
\end{table*}

\clearpage

\begin{table*}
\centering
\setlength{\tabcolsep}{4pt}
\caption{Coarse-to-fine gating comparison. The \textbf{best value} is in bold and \underline{the second-best value} is indicated with an underline.}
% \vspace{-6pt}
\resizebox{0.5\linewidth}{!}{
\begin{tabular}{ccccc}
\toprule
\multirow{2}{*}{\# Method} & \multirow{2}{*}{Text-alignment} & \multicolumn{3}{c}{Image-alignment}\\
\cmidrule(lr){3-5}
& & Concept 1
& Concept 2
& Concept 3 \\
\midrule
m-\our & 0.731 & 0.719 & 0.714 & 0.747 \\
l-\our & \underline{0.760} & \underline{0.727} & \underline{0.731} & \textbf{0.757} \\
\hc b-\our & \textbf{0.766} & 0.726 & \textbf{0.737} & \underline{0.755} \\
n-\our & 0.722 & \textbf{0.739} & 0.682 & 0.730 \\
\bottomrule
\end{tabular}}
\label{tb: c-to-f MOLE}
\vspace{-2mm}
\end{table*}

\begin{table*}
\centering
\setlength{\tabcolsep}{5pt}
\caption{Experimental results on the impact of exploring expand expert numbers on model performance. We evaluate each composition pair on $200$ images generated using $5$ prompts with $50$ steps of DDPM sampler and scale=$7.5$. NLA denotes normalized linear arithmetic composition (Eq.~\ref{Eq.Normalize-Combination}). The best performance is in bold.}
\resizebox{0.55\linewidth}{!}{
\begin{tabular}{ccccccc}
\toprule
\multirow{2}{*}{\# Number of LoRA} & \multicolumn{3}{c}{Text-alignment} & \multicolumn{3}{c}{Average Image-alignment} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
\tiny & NLA & SVDiff & \our & NLA & SVDiff & \our \\
\midrule
3 & 0.678 & 0.728 & \textbf{0.759} %
& 0.694 & 0.719 & \textbf{0.757} \\

4 & 0.681 & 0.717 & \textbf{0.725} %
& 0.712 & 0.721 & \textbf{0.742} \\

5 & 0.652 & 0.723 & \textbf{0.762} %
& 0.682 & 0.708 & \textbf{0.737} \\

6 & 0.698 & 0.709 & \textbf{0.737} %
& 0.703 & 0.701 & \textbf{0.709} \\
\hc Average & 0.677 & 0.719 & \textbf{0.746} & 0.698 & 0.712 & \textbf{0.736} \\
\bottomrule
\end{tabular}
}
\label{tb: VL_large_lora}
\end{table*}


\begin{figure*}[!tb]
\centering
\includegraphics[width=\linewidth]{retain_ability.pdf} %
\vspace{-5mm}
\caption{Qualitative result for retaining ability experiment. NLA denotes normalized linear arithmetic composition (Eq.~\ref{Eq.Normalize-Combination}). The first row displays the composed trained LoRAs. The second to the last row showcases the respective abilities of different composition methods to preserve the characteristics of each LoRA without altering the model.}
\label{fig:retain_ability}
\vspace{-2mm}
\end{figure*}
%
\begin{figure*}[!tb]
\centering
\includegraphics[width=\linewidth]{NLP_gating_div_vis.png}\\
\vspace{-1mm}
% \makebox[0.05\linewidth]{\makecell{\scriptsize LoRA \\ \scriptsize experts}}
\hfill
\makebox[0.11\linewidth]{\scriptsize Gating 2}
\makebox[0.11\linewidth]{\scriptsize Gating 5}
\makebox[0.11\linewidth]{\scriptsize Gating 8}
\makebox[0.11\linewidth]{\scriptsize Gating 11}
\makebox[0.11\linewidth]{\scriptsize Gating 14}
\makebox[0.11\linewidth]{\scriptsize Gating 17}
\makebox[0.11\linewidth]{\scriptsize Gating 20}
\makebox[0.11\linewidth]{\scriptsize Gating 23}
\\
\vspace{1mm}
\includegraphics[width=\linewidth]{VL_gating_div_vis.png} \\
\vspace{-1mm}
\hfill
\makebox[0.11\linewidth]{\scriptsize Gating 1}
\makebox[0.11\linewidth]{\scriptsize Gating 2}
\makebox[0.11\linewidth]{\scriptsize Gating 3}
\makebox[0.11\linewidth]{\scriptsize Gating 4}
\makebox[0.11\linewidth]{\scriptsize Gating 5}
\makebox[0.11\linewidth]{\scriptsize Gating 6}
\makebox[0.11\linewidth]{\scriptsize Gating 7}
\makebox[0.11\linewidth]{\scriptsize Gating 8}
\vspace{-2mm}
\caption{Visualization of the weights (\%) predicted by each gating function (horizontal axis) for LoRA experts (vertical axis) during inference. The top row corresponds to experiments in the NLP domain, while the bottom row pertains to experiments in the V\&L domain.}
\label{fig:gating_div_vis}
% \vspace{-15mm}
\end{figure*}
%
%
\clearpage
%
%
%
\begin{figure*}[htbp]
\centering
\includegraphics[width=\linewidth]{retain_ability2.pdf} %
\vspace{-2mm}
\caption{Visualization for different inference modes of \our. \our~has two inference modes: In the first mode~(the first line), \our~can use all the LoRA experts and allocate weights for each LoRA, preserving their individual characteristics. In the second mode~(the second and third lines), we can manually mask some unwanted LoRAs without changing the gating weights. It can recalculate and distribute weights proportionally.
%
These two modes enable \our~to adapt to different scenarios, providing a versatile and flexible approach for effective LoRA composition.}
\label{fig:retain_ability2}
% \vspace{-18mm}
\end{figure*}
%


\begin{figure*}[htbp]
\centering
\includegraphics[width=\linewidth]{VL_main_pic.pdf} %
\vspace{-4mm}
\caption{Visualization of multiple LoRA composition results on V\&L domain. NLA denotes normalized linear arithmetic composition (Eq.~\ref{Eq.Normalize-Combination}). Our \our~has higher visual similarity with the personal cat and dog images while following the text condition better,~e.g., SVDiff is unable to fully recover all the characteristics of LoRA (in the second line, the appearance of the dog is completely altered, and in the first line, two cats are present but the dog is missing). Moreover, SVDiff and NLA struggles to generate images that match the text condition effectively (e.g., it might add sunglasses to both dogs and cats in response to conditions mentioning ``dog'' and ``cat'').}
\label{fig:VL_main_pic}
\vspace{-4mm}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=\linewidth]{VL_main_pic2.pdf} %
\vspace{-4mm}
\caption{Visualization of multiple LoRA composition results on V\&L domain. NLA denotes normalized linear arithmetic composition (Eq.~\ref{Eq.Normalize-Combination}). Our model consistently produces results that better align with the prompt descriptions. The outputs from our model consistently contain all three visual concepts that need to be combined. In contrast, SVDiff and NLA often exhibit issues such as concept confusion (e.g., in the third row of NLA, where features of both the cat and dog are confused) and concept omission (e.g., in the second row of SVDiff, where the concept of the dog is missing, and in the first row, where the concept of the cat is missing).}
\label{fig:VL_main_pic2}
\vspace{-4mm}
\end{figure*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=\linewidth]{VL_main_pic3.pdf} %
\vspace{-4mm}
\caption{Visualization of multiple LoRA composition results on V\&L domain. NLA denotes normalized linear arithmetic composition (Eq.~\ref{Eq.Normalize-Combination}). Our model consistently produces results that better align with the prompt descriptions. The outputs from our model consistently contain all three visual concept features that need to be combined. In contrast, SVDiff and NLA often exhibit issues such as concept omission (e.g., in the first row of NLA, where the concepts of the cat and sunglasses are missing, and in the first row of SVDiff, where the concept of sunglasses is missing). Additionally, our output results better match the original visual concept features. For example, the shell of the turtle is green, whereas SVDiff and NLA generate shells in pink and brown colors.}
\label{fig:VL_main_pic3}
\vspace{-4mm}
\end{figure*}

\end{document}
